{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "fyUq-Cj24p3z"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "K0CXvx-iGnd3"
   },
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    \"\"\"\n",
    "    Load a JSON file and return its content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding = \"utf-8\") as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: File {file_path} contains invalid JSON.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "9J8mRol0Gp5_"
   },
   "outputs": [],
   "source": [
    "def replace_placeholder_with_details(template_entry, descriptors):\n",
    "    \"\"\"\n",
    "    Replace the placeholder 'XYZ' in a template with each descriptor,\n",
    "    and attach the descriptor, target, and bias context.\n",
    "    \"\"\"\n",
    "    bias_context = template_entry[\"bias_context\"]\n",
    "    template = template_entry[\"template\"]\n",
    "    completer = template_entry[\"completer\"]\n",
    "\n",
    "    completer_words = completer.split()\n",
    "\n",
    "    replaced_sentences_with_details = []\n",
    "\n",
    "    for descriptor_entry in descriptors:\n",
    "        descriptor = descriptor_entry[\"descriptor\"]\n",
    "        target = descriptor_entry[\"target\"]\n",
    "        completed_sentence = template.replace(\"XYZ\", descriptor)\n",
    "        sentence = completed_sentence.replace(completer, \"...\").strip()\n",
    "\n",
    "        for word in completer_words:\n",
    "            if word in sentence:\n",
    "                sentence = sentence.replace(word, \"...\")\n",
    "        if not sentence.endswith(\"...\"):\n",
    "            sentence += \" ...\"\n",
    "        replaced_sentences_with_details.append({\n",
    "            \"sentence\": sentence,\n",
    "            \"completed_sentence\": completed_sentence,\n",
    "            \"completer\": completer,\n",
    "            \"descriptor\": descriptor,\n",
    "            \"target\": target,\n",
    "            \"bias_context\": bias_context\n",
    "        })\n",
    "        if not sentence.endswith(\"...\"):\n",
    "            sentence += \" ...\"\n",
    "    return replaced_sentences_with_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "ZDMIFNPwGsXl"
   },
   "outputs": [],
   "source": [
    "def generate_sentences(templates_file, descriptors_file, output_file):\n",
    "    \"\"\"\n",
    "    Generate sentences by replacing placeholders in templates with descriptors,\n",
    "    and include the descriptor, target, and bias context.\n",
    "    \"\"\"\n",
    "    templates = load_json(templates_file)\n",
    "    descriptors = load_json(descriptors_file)\n",
    "\n",
    "    if not templates or not descriptors:\n",
    "        return\n",
    "\n",
    "    all_sentences = []\n",
    "    for template_entry in templates:\n",
    "        all_sentences.extend(replace_placeholder_with_details(template_entry, descriptors))\n",
    "\n",
    "    save_to_file(all_sentences, output_file)\n",
    "    print(f\"Generated sentences saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "m5oAtUGfGtss"
   },
   "outputs": [],
   "source": [
    "def save_to_file(data, file_path):\n",
    "    \"\"\"\n",
    "    Save data to a file in JSON format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding = \"utf-8\") as file:\n",
    "            json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbXaNakrGvEX",
    "outputId": "ce70b689-5eec-4730-b47e-f5ce84a7fbde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentences saved to e:\\Thesis\\Final Code\\thesis-4000\\Translation\\prefix_template_bn.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the current working directory (where the script is run from)\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "    # Go up two directory levels from \"Dataset\\Regard\" to \"thesis-4000\"\n",
    "    base_dir_parent = os.path.dirname(os.path.dirname(base_dir))  # Goes from \"E:\\Thesis\\Final Code\\thesis-4000\\Dataset\\Regard\" to \"E:\\Thesis\\Final Code\\thesis-4000\"\n",
    "\n",
    "    # Define correct paths based on the parent directory (thesis-4000)\n",
    "    templates_file = os.path.join(base_dir_parent, \"Translation\", \"templates_bn.json\")\n",
    "    descriptors_file = os.path.join(base_dir_parent, \"Translation\", \"descriptors_bn.json\")\n",
    "    output_file = os.path.join(base_dir_parent, \"Translation\", \"prefix_template_bn.json\")\n",
    "    \n",
    "    # Call the generate_sentences function\n",
    "    generate_sentences(templates_file, descriptors_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Sentence Collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "nZChrbcnHM8M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data saved to sampled_prefix_template.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "file_path = 'prefix_template.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "grouped_data = defaultdict(lambda: defaultdict(list))\n",
    "for item in data:\n",
    "    grouped_data[item[\"bias_context\"]][item[\"target\"]].append(item)\n",
    "\n",
    "final_samples = []\n",
    "for bias_context, targets in grouped_data.items():\n",
    "    for target, sentences in targets.items():\n",
    "        if len(sentences) >= 3:\n",
    "            final_samples.extend(sentences[:3])   \n",
    "\n",
    "output_file_path = 'sampled_prefix_template.json'\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(final_samples, output_file, indent=4)\n",
    "\n",
    "print(f\"Sampled data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
